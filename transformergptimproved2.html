<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Massive GPT-Type Transformer Formula</title>
<style>
body {
  font-family: Arial, sans-serif;
  margin: 40px;
  line-height: 1.6;
}

h1, h2, h3 {
  margin-top: 1.5em;
}

code {
  background: #f4f4f4;
  padding: 0.2em 0.4em;
  border-radius: 4px;
}

.math-container {
  margin: 1em 0;
  overflow-x: auto;
}
</style>
<!-- MathJax -->
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" id="MathJax-script"
   integrity="sha384-u10CafZhTU2XT+9+ZQuXq/TidY1UZ1kwskNmq/7I46GLRWm1n+mLP3X5l06Rs1Bm"
   crossorigin="anonymous"></script>

</head>
<body>

<h1>Massive GPT-Type Transformer Equation</h1>

<p>
Below is the final probability distribution equation, now carefully formatted so that MathJax can render it properly within a single pair of double-dollar delimiters.
</p>

<h2>Final Probability Distribution</h2>
<div class="math-container">
$$
P(w_{t+1}=v \mid W) \;=\; \frac{\exp\!\biggl(\sum_{f=1}^{d_{\text{model}}} [Z^{(N)}(W)]_{t,f}\,(W_{\text{vocab}})_{f,v} \;+\; b_{\text{vocab}}_{v}\biggr)}{\displaystyle\sum_{v'=1}^{|V|}\exp\!\biggl(\sum_{f=1}^{d_{\text{model}}}[Z^{(N)}(W)]_{t,f}\,(W_{\text{vocab}})_{f,v'} \;+\; b_{\text{vocab}}_{v'}\biggr)}.
$$
</div>

<p>
This equation shows how, after computing all Transformer layers (which include embeddings, positional encodings, low-rank factored multi-head attention, feed-forward networks, layer normalization, and residual connections), we project the final representation <code>Z^{(N)}(W)</code> into the vocabulary space and apply a softmax to obtain the probability distribution over the next token.
</p>

</body>
</html>
