<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Top 60 AGI/ASI Models - Updated 04:22 UKTIME 21/12/2024</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
    }
    h1, h2 {
      margin-bottom: 0.5em;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      max-width: 1000px;
    }
    th, td {
      border: 1px solid #aaa;
      padding: 8px 12px;
      text-align: left;
    }
    th {
      background-color: #ddd;
    }
    .notes {
      font-size: 0.9em;
      color: #444;
    }
  </style>
</head>
<body>
  <h1>Top 60 Models</h1>
  <p><strong>Date/Time:</strong> 04:22 UKTIME 21/12/2024</p>
  <p class="notes">
    The first three models (<em>o3</em>, <em>o3 mini</em>, and 
    <em>ChatGPT o1 Pro mode</em>) are labeled as <strong>early ASI</strong>. 
    The <em>DeepSeek Chain of Thought</em> model is placed ordinally at rank 6 
    (no numeric score). All other models are from the LiveBench Global Average 
    ranking or estimated placements.
  </p>

  <table>
    <thead>
      <tr>
        <th>Rank</th>
        <th>Model</th>
        <th>Score / Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>1</td><td>o3 <em>(early ASI)</em></td><td>—</td></tr>
      <tr><td>2</td><td>o3 mini <em>(early ASI)</em></td><td>—</td></tr>
      <tr><td>3</td><td>ChatGPT o1 Pro mode <em>(early ASI)</em></td><td>Estimated 80.00</td></tr>
      <tr><td>4</td><td>ChatGPT o1 (o1-2024-12-17)</td><td>75.67</td></tr>
      <tr><td>5</td><td>ChatGPT o1 Preview (o1-preview-2024-09-12)</td><td>65.79</td></tr>
      <tr><td>6</td><td>DeepSeek Chain of Thought</td><td>No Score (Ordinal Placement)</td></tr>
      <tr><td>7</td><td>Gemini Pro 2.0 1206 (gemini-exp-1206)</td><td>64.09</td></tr>
      <tr><td>8</td><td>Gemini 2.0 Flash Thinking Exp (gemini-2.0-flash-thinking-exp-1219)</td><td>61.57</td></tr>
      <tr><td>9</td><td>Gemini 2.0 Flash Exp (gemini-2.0-flash-exp)</td><td>59.26</td></tr>
      <tr><td>10</td><td>Claude 3.5 Sonnet (20241022)</td><td>59.03</td></tr>
      <tr><td>11</td><td>Claude 3.5 Sonnet (20240620)</td><td>58.74</td></tr>
      <tr><td>12</td><td>ChatGPT o1 mini (o1-mini-2024-09-12)</td><td>57.76</td></tr>
      <tr><td>13</td><td>Gemini Exp 1121 (gemini-exp-1121)</td><td>57.36</td></tr>
      <tr><td>14</td><td>GPT-4o (gpt-4o-2024-08-06)</td><td>55.33</td></tr>
      <tr><td>15</td><td>GPT-4o (gpt-4o-2024-05-13)</td><td>54.41</td></tr>
      <tr><td>16</td><td>Gemini 1.5 Pro (gemini-1.5-pro-002)</td><td>54.33</td></tr>
      <tr><td>17</td><td>Grok (grok-2-1212)</td><td>54.30</td></tr>
      <tr><td>18</td><td>Gemini 1.5 Pro Exp (gemini-1.5-pro-exp-0827)</td><td>53.29</td></tr>
      <tr><td>19</td><td>Meta Llama 3.1 405b Turbo (meta-llama-3.1-405b-instruct-turbo)</td><td>52.36</td></tr>
      <tr><td>20</td><td>GPT-4o (gpt-4o-2024-11-20)</td><td>52.19</td></tr>
      <tr><td>21</td><td>LearnLM 1.5 Pro Experimental (learnlm-1.5-pro-experimental)</td><td>52.19</td></tr>
      <tr><td>22</td><td>ChatGPT-4o-latest-0903</td><td>51.66</td></tr>
      <tr><td>23</td><td>Qwen2.5-72b-Instruct-Turbo</td><td>51.44</td></tr>
      <tr><td>24</td><td>GPT-4 Turbo (gpt-4-turbo-2024-04-09)</td><td>50.40</td></tr>
      <tr><td>25</td><td>Llama 3.3 70b Turbo (llama-3.3-70b-instruct-turbo)</td><td>50.16</td></tr>
      <tr><td>26</td><td>Grok Beta (grok-beta)</td><td>49.18</td></tr>
      <tr><td>27</td><td>Claude 3 Opus (claude-3-opus-20240229)</td><td>49.12</td></tr>
      <tr><td>28</td><td>Gemini 1.5 Flash (gemini-1.5-flash-002)</td><td>48.59</td></tr>
      <tr><td>29</td><td>Mistral Large 2411 (mistral-large-2411)</td><td>48.43</td></tr>
      <tr><td>30</td><td>Mistral Large 2407 (mistral-large-2407)</td><td>48.31</td></tr>
      <tr><td>31</td><td>Qwen2.5-Coder-32b-Instruct</td><td>46.23</td></tr>
      <tr><td>32</td><td>DeepSeek v2.5 (deepseek-v2.5-1210)</td><td>45.98</td></tr>
      <tr><td>33</td><td>GPT-4-0125-preview (gpt-4-0125-preview)</td><td>45.71</td></tr>
      <tr><td>34</td><td>Gemini 1.5 Flash Exp (gemini-1.5-flash-exp-0827)</td><td>45.21</td></tr>
      <tr><td>35</td><td>Meta Llama 3.1 70b Turbo (meta-llama-3.1-70b-instruct-turbo)</td><td>44.89</td></tr>
      <tr><td>36</td><td>Gemini 1.5 Pro 001 (gemini-1.5-pro-001)</td><td>44.22</td></tr>
      <tr><td>37</td><td>Amazon Nova Pro (amazon.nova-pro-v1:0)</td><td>43.55</td></tr>
      <tr><td>38</td><td>Claude 3.5 Haiku (20241022)</td><td>43.45</td></tr>
      <tr><td>39</td><td>GPT-4o Mini (gpt-4o-mini-2024-07-18)</td><td>41.26</td></tr>
      <tr><td>40</td><td>Qwq-32b-preview (qwq-32b-preview)</td><td>39.90</td></tr>
      <tr><td>41</td><td>Gemini 1.5 Flash 001 (gemini-1.5-flash-001)</td><td>39.22</td></tr>
      <tr><td>42</td><td>Gemma 2 27b It (gemma-2-27b-it)</td><td>38.19</td></tr>
      <tr><td>43</td><td>Gemini 1.5 Flash 8b Exp 0827 (gemini-1.5-flash-8b-exp-0827)</td><td>36.67</td></tr>
      <tr><td>44</td><td>Amazon Nova Lite (amazon.nova-lite-v1:0)</td><td>36.35</td></tr>
      <tr><td>45</td><td>Gemini 1.5 Flash 8b Exp 0924 (gemini-1.5-flash-8b-exp-0924)</td><td>36.01</td></tr>
      <tr><td>46</td><td>Qwen2.5-7b-Instruct-Turbo</td><td>34.90</td></tr>
      <tr><td>47</td><td>Claude 3 Haiku (20240307)</td><td>33.85</td></tr>
      <tr><td>48</td><td>Mistral Small 2409 (mistral-small-2409)</td><td>33.39</td></tr>
      <tr><td>49</td><td>Mixtral-8x22b-Instruct-v0.1 (mixtral-8x22b-instruct-v0.1)</td><td>32.45</td></tr>
      <tr><td>50</td><td>Command-r-plus-08-2024 (command-r-plus-08-2024)</td><td>31.76</td></tr>
      <tr><td>51</td><td>Amazon Nova Micro (amazon.nova-micro-v1:0)</td><td>29.56</td></tr>
      <tr><td>52</td><td>Gemma 2 9b It (gemma-2-9b-it)</td><td>28.66</td></tr>
      <tr><td>53</td><td>Mistral Small 2402 (mistral-small-2402)</td><td>28.36</td></tr>
      <tr><td>54</td><td>Command-r-08-2024 (command-r-08-2024)</td><td>27.31</td></tr>
      <tr><td>55</td><td>Command-r-plus-04-2024 (command-r-plus-04-2024)</td><td>27.11</td></tr>
      <tr><td>56</td><td>Meta Llama 3.1 8b Turbo (meta-llama-3.1-8b-instruct-turbo)</td><td>25.97</td></tr>
      <tr><td>57</td><td>Phi-3-Small-8k-Instruct (phi-3-small-8k-instruct)</td><td>24.03</td></tr>
      <tr><td>58</td><td>Phi-3-Mini-128k-Instruct (phi-3-mini-128k-instruct)</td><td>22.36</td></tr>
      <tr><td>59</td><td>Olmo-2-1124-13b-Instruct (olmo-2-1124-13b-instruct)</td><td>22.09</td></tr>
      <tr><td>60</td><td>Phi-3-Mini-4k-Instruct (phi-3-mini-4k-instruct)</td><td>22.08</td></tr>
    </tbody>
  </table>

  <p class="notes">
    This HTML file shows the top 60 entries of the combined ranking as requested, 
    with o3, o3 mini, and ChatGPT o1 Pro mode labeled as early ASI. The date 
    has been corrected to 21/12/2024 as requested.
  </p>

  <script>
    // Timer to measure compute performance
    const startTime = performance.now();
    window.addEventListener('load', () => {
      const endTime = performance.now();
      const totalTime = endTime - startTime;
      console.log("Page load time: " + totalTime.toFixed(2) + " ms");
    });
  </script>
</body>
</html>
