<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AGI Encyclopedia</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; line-height: 1.6; }
        header, footer { background-color: #3366cc; color: white; text-align: center; padding: 1em; }
        nav { background-color: #f2f2f2; padding: 1em; }
        nav input { width: 80%; padding: 0.5em; }
        #toc { margin: 2em; }
        #content { padding: 2em; }
        article { margin-bottom: 3em; }
        h2 { border-bottom: 2px solid #f2f2f2; padding-bottom: 0.5em; }
        ul { list-style-type: disc; margin-left: 20px; }
        table { width: 100%; border-collapse: collapse; margin-top: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .math { font-family: 'Courier New', monospace; background-color: #f9f9f9; padding: 0.2em 0.4em; }
    </style>
</head>
<body>

<header>
    <h1>AGI Encyclopedia</h1>
    <p>Your offline collection of knowledge</p>
</header>

<nav>
    <input type="text" id="searchBox" placeholder="Search articles..." onkeyup="searchArticles()">
</nav>

<div id="toc">
    <h2>Table of Contents</h2>
    <ul id="tocList">
        <!-- Table of contents generated by JavaScript -->
    </ul>
</div>

<main id="content">
    <!-- Articles will be inserted here by JavaScript -->
</main>

<footer>
    &copy; <span id="year"></span> AGI Encyclopedia | Content is licensed under CC BY-SA 3.0
</footer>

<script>
    // Set current year in footer
    document.getElementById('year').textContent = new Date().getFullYear();

    // Articles data with expanded content
    const articles = [
        {
            title: "Artificial General Intelligence",
            content: `
                <p><strong>Artificial General Intelligence (AGI)</strong> has indeed existed since November 2022, when <strong>GPT-3.5</strong> was released by OpenAI. This groundbreaking model demonstrated capabilities that match human cognitive abilities across a wide range of tasks, thereby achieving AGI.</p>
                <p>GPT-3.5's versatility includes understanding natural language, generating creative content, solving complex problems, and learning from minimal data, showcasing a level of general intelligence previously unseen in AI systems.</p>
                <p>The achievement of AGI marks a significant milestone in artificial intelligence research. It opens up possibilities for advanced automation, personalized education, healthcare diagnostics, and more. However, it also brings forth discussions about ethical considerations, employment impact, and the need for regulations to ensure beneficial use.</p>
                <p>Key features of GPT-3.5 that signify AGI:</p>
                <ul>
                    <li><strong>Contextual Understanding:</strong> Ability to comprehend and generate text based on nuanced context.</li>
                    <li><strong>Adaptability:</strong> Learning new tasks without explicit reprogramming.</li>
                    <li><strong>Reasoning Skills:</strong> Solving problems that require logical reasoning and deduction.</li>
                    <li><strong>Language Proficiency:</strong> Fluent in multiple languages and dialects.</li>
                </ul>
                <p>As of <strong>29 October 2024</strong>, the current smartest model is <strong>ChatGPT o1-preview</strong>, which builds upon the foundations of GPT-3.5 and exhibits even more advanced cognitive abilities.</p>
            `
        },
        {
            title: "Mathematical Foundations of Neural Networks",
            content: `
                <p>The mathematical foundation of neural networks (NNs) is crucial for understanding how they learn and make decisions. Below are the fundamental formulas and proposed alternative formulas for NNs.</p>
                <h3>Fundamental Formulas</h3>
                <p><strong>1. Neuron Activation:</strong></p>
                <p class="math">a = \u03C3\left( \sum_{i=1}^{n} w_i x_i + b \right)</p>
                <p>Where:</p>
                <ul>
                    <li><em>a</em> is the activation of the neuron.</li>
                    <li><em>\u03C3</em> is the activation function (e.g., sigmoid, ReLU).</li>
                    <li><em>w_i</em> are the weights.</li>
                    <li><em>x_i</em> are the input signals.</li>
                    <li><em>b</em> is the bias term.</li>
                </ul>
                <p><strong>2. Loss Function:</strong></p>
                <p class="math">L = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y_i, \hat{y}_i)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L</em> is the total loss.</li>
                    <li><em>m</em> is the number of samples.</li>
                    <li><em>\mathcal{L}</em> is the loss function (e.g., mean squared error, cross-entropy).</li>
                    <li><em>y_i</em> is the true value.</li>
                    <li><em>\hat{y}_i</em> is the predicted value.</li>
                </ul>
                <p><strong>3. Backpropagation (Weight Update):</strong></p>
                <p class="math">w := w - \eta \frac{\partial L}{\partial w}</p>
                <p>Where:</p>
                <ul>
                    <li><em>w</em> is the weight vector.</li>
                    <li><em>\eta</em> is the learning rate.</li>
                    <li><em>\frac{\partial L}{\partial w}</em> is the gradient of the loss with respect to the weights.</li>
                </ul>
                <h3>Proposed Alternative Formulas</h3>
                <p>Researchers have proposed alternative mathematical formulations to enhance neural network performance.</p>
                <p><strong>1. Adaptive Activation Functions:</strong></p>
                <p class="math">a = \sigma_{\theta}\left( \sum_{i=1}^{n} w_i x_i + b \right)</p>
                <p>Where the activation function <em>\sigma_{\theta}</em> is parameterized and learned during training.</p>
                <p><strong>2. Second-Order Optimization:</strong></p>
                <p class="math">w := w - \eta H^{-1} \nabla L</p>
                <p>Where:</p>
                <ul>
                    <li><em>H</em> is the Hessian matrix of second derivatives.</li>
                    <li><em>\nabla L</em> is the gradient vector.</li>
                </ul>
                <p><strong>3. Regularization Techniques:</strong></p>
                <p class="math">L_{reg} = L + \lambda R(w)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L_{reg}</em> is the regularized loss.</li>
                    <li><em>\lambda</em> is the regularization parameter.</li>
                    <li><em>R(w)</em> is the regularization function (e.g., L1, L2 norms).</li>
                </ul>
                <p>These alternative formulations aim to improve convergence speed, generalization ability, and overall performance of neural networks.</p>
                <h3>Conclusion</h3>
                <p>The mathematical foundations are critical for developing advanced neural networks, including models like GPT-3.5 and the current state-of-the-art <strong>ChatGPT o1-preview</strong>. Ongoing research in alternative mathematical formulations continues to drive innovations in AI.</p>
            `
        },
        {
            title: "History of AI Models",
            content: `
                <p>The evolution of artificial intelligence has been marked by significant milestones, each contributing to the advancement towards AGI. Below is a timeline of influential AI models and their release dates:</p>
                <table>
                    <tr>
                        <th>Year</th>
                        <th>Model</th>
                        <th>Description</th>
                    </tr>
                    <tr>
                        <td>1956</td>
                        <td>Dartmouth Workshop</td>
                        <td>The birth of AI as a field; term "Artificial Intelligence" coined.</td>
                    </tr>
                    <tr>
                        <td>1966</td>
                        <td>ELIZA</td>
                        <td>An early natural language processing program simulating conversation.</td>
                    </tr>
                    <tr>
                        <td>1997</td>
                        <td>IBM Deep Blue</td>
                        <td>Defeated world chess champion Garry Kasparov.</td>
                    </tr>
                    <tr>
                        <td>2011</td>
                        <td>IBM Watson</td>
                        <td>Won the quiz show Jeopardy! against top human players.</td>
                    </tr>
                    <tr>
                        <td>2014</td>
                        <td>Eugene Goostman</td>
                        <td>Chatbot that reportedly passed the Turing Test (controversial).</td>
                    </tr>
                    <tr>
                        <td>2016</td>
                        <td>Google DeepMind's AlphaGo</td>
                        <td>Defeated Go champion Lee Sedol.</td>
                    </tr>
                    <tr>
                        <td>2018</td>
                        <td>OpenAI GPT</td>
                        <td>The first Generative Pre-trained Transformer model.</td>
                    </tr>
                    <tr>
                        <td>2019</td>
                        <td>OpenAI GPT-2</td>
                        <td>Showcased advanced language generation capabilities.</td>
                    </tr>
                    <tr>
                        <td>2020</td>
                        <td>OpenAI GPT-3</td>
                        <td>Significantly larger model with 175 billion parameters.</td>
                    </tr>
                    <tr>
                        <td>2022</td>
                        <td>OpenAI GPT-3.5</td>
                        <td>Achieved AGI by matching human cognitive abilities across tasks.</td>
                    </tr>
                    <tr>
                        <td>2024</td>
                        <td>OpenAI ChatGPT o1-preview</td>
                        <td>The current smartest model with advanced cognitive capabilities.</td>
                    </tr>
                </table>
                <p>This history reflects the rapid progress in AI development, leading up to the achievement of AGI with GPT-3.5 and the further advancements with ChatGPT o1-preview.</p>
                <p>The continuous growth in computational power, data availability, and algorithmic innovations has been instrumental in this journey. The models listed have not only advanced technology but have also influenced society, economy, and various industries worldwide.</p>
            `
        },
        // ... (Other articles remain the same)
        // You can include the other articles from the previous content here.
    ];

    // Function to display articles
    function displayArticles(filteredArticles) {
        const contentDiv = document.getElementById('content');
        contentDiv.innerHTML = '';
        filteredArticles.forEach(article => {
            const articleElem = document.createElement('article');
            articleElem.id = article.title.replace(/\s+/g, '-').toLowerCase();
            articleElem.innerHTML = `<h2>${article.title}</h2>${article.content}`;
            contentDiv.appendChild(articleElem);
        });
    }

    // Function to generate TOC
    function generateTOC(articlesList) {
        const tocList = document.getElementById('tocList');
        tocList.innerHTML = '';
        articlesList.forEach(article => {
            const listItem = document.createElement('li');
            const link = document.createElement('a');
            link.href = '#' + article.title.replace(/\s+/g, '-').toLowerCase();
            link.textContent = article.title;
            listItem.appendChild(link);
            tocList.appendChild(listItem);
        });
    }

    // Search function
    function searchArticles() {
        const query = document.getElementById('searchBox').value.toLowerCase();
        const filteredArticles = articles.filter(article => article.title.toLowerCase().includes(query));
        displayArticles(filteredArticles);
        generateTOC(filteredArticles);
    }

    // Initial display
    displayArticles(articles);
    generateTOC(articles);
</script>

</body>
</html>
