<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AGI Encyclopedia</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; line-height: 1.6; }
        header, footer { background-color: #3366cc; color: white; text-align: center; padding: 1em; }
        nav { background-color: #f2f2f2; padding: 1em; }
        nav input { width: 80%; padding: 0.5em; }
        #toc { margin: 2em; }
        #content { padding: 2em; }
        article { margin-bottom: 3em; }
        h2 { border-bottom: 2px solid #f2f2f2; padding-bottom: 0.5em; }
        ul { list-style-type: disc; margin-left: 20px; }
        table { width: 100%; border-collapse: collapse; margin-top: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .math { font-family: 'Courier New', monospace; background-color: #f9f9f9; padding: 0.2em 0.4em; display: inline-block; }
    </style>
    <!-- MathJax for rendering LaTeX formulas -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>

<header>
    <h1>AGI Encyclopedia</h1>
    <p>Your offline collection of knowledge</p>
</header>

<nav>
    <input type="text" id="searchBox" placeholder="Search articles..." onkeyup="searchArticles()">
</nav>

<div id="toc">
    <h2>Table of Contents</h2>
    <ul id="tocList">
        <!-- Table of contents generated by JavaScript -->
    </ul>
</div>

<main id="content">
    <!-- Articles will be inserted here by JavaScript -->
</main>

<footer>
    &copy; <span id="year"></span> AGI Encyclopedia | Content is licensed under CC BY-SA 3.0
</footer>

<script>
    // Set current year in footer
    document.getElementById('year').textContent = new Date().getFullYear();

    // Articles data with expanded content
    const articles = [
        {
            title: "Artificial General Intelligence",
            content: `
                <p><strong>Artificial General Intelligence (AGI)</strong> has indeed existed since November 2022, when <strong>GPT-3.5</strong> was released by OpenAI. This groundbreaking model demonstrated capabilities that match human cognitive abilities across a wide range of tasks, thereby achieving AGI.</p>
                <p>In March 2023, OpenAI released <strong>GPT-4</strong>, further advancing AGI capabilities with enhanced reasoning, problem-solving skills, and broader general knowledge. GPT-4 solidified OpenAI's position as the leader in AGI development.</p>
                <p>OpenAI maintained a near monopoly on this level of intelligence until 2024, when it was overtaken and matched by a series of other GPT-4 models developed by different organizations. The most notable among these was <strong>Claude 3.5 Sonnet</strong> by Anthropic, which, until recently, ranked first in worldwide AI model rankings.</p>
                <p>As of <strong>29 October 2024</strong>, the current smartest model is <strong>ChatGPT o1-preview</strong>, which has taken the top spot, pushing Claude 3.5 Sonnet to second place.</p>
                <h3>AI Model Rankings</h3>
                <p>The following table lists the top AI models as of <strong>Wednesday, 23 October 2024</strong>:</p>
                <table>
                    <tr>
                        <th>Rank</th>
                        <th>Model</th>
                        <th>Average Score</th>
                        <th>Benchmark Details</th>
                    </tr>
                    <tr>
                        <td>1st</td>
                        <td>ChatGPT o1-preview</td>
                        <td>83.20%</td>
                        <td>Average of (73.3% + 85.5% + 90.8%) / 3</td>
                    </tr>
                    <tr>
                        <td>2nd</td>
                        <td>Claude 3.5 Sonnet (New)</td>
                        <td>71.64%</td>
                        <td>Average of sub-values from images & categorical benchmarks</td>
                    </tr>
                    <tr>
                        <td>3rd</td>
                        <td>Claude 3.5 Sonnet (Old)</td>
                        <td>66.86%</td>
                        <td>Average of all sub-values from images, categorical & table</td>
                    </tr>
                    <tr>
                        <td>4th</td>
                        <td>Grok-2+</td>
                        <td>76.53%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>5th</td>
                        <td>Llama 3 405B</td>
                        <td>76.07%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>6th</td>
                        <td>GPT-4o</td>
                        <td>75.93%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>7th</td>
                        <td>Grok-2 mini+</td>
                        <td>74.05%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>8th</td>
                        <td>GPT-4 Turbo</td>
                        <td>70.79%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>9th</td>
                        <td>Gemini Pro 1.5</td>
                        <td>69.99%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>10th</td>
                        <td>Llama 3.2 90B</td>
                        <td>69.18%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>11th</td>
                        <td>Claude 3 Opus</td>
                        <td>68.60%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>12th</td>
                        <td>Qwen2.5-72B Instruct</td>
                        <td>65.38%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>13th</td>
                        <td>GPT-4o-mini</td>
                        <td>59.29%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>14th</td>
                        <td>Llama 3.2 11B</td>
                        <td>60.67%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>15th</td>
                        <td>Grok-1.5</td>
                        <td>60.61%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>16th</td>
                        <td>Claude 3 Haiku</td>
                        <td>56.70%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                </table>
                <p>Note: For most of 2024, <strong>Claude 3.5 Sonnet</strong> held the first position until recently, when <strong>ChatGPT o1-preview</strong> surpassed it.</p>
            `
        },
        {
            title: "Mathematical Foundations of Neural Networks",
            content: `
                <p>The mathematical foundation of neural networks (NNs) is crucial for understanding how they learn and make decisions. Below are the fundamental formulas and proposed alternative formulas for NNs. The formulas are presented in LaTeX and plain text versions.</p>
                <h3>Fundamental Formulas</h3>
                <p><strong>1. Neuron Activation:</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅa = \sigma\left( \sum_{i=1}^{n} w_i x_i + b \right)ÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">a = œÉ( Œ£ w_i x_i + b )</p>
                <p>Where:</p>
                <ul>
                    <li><em>a</em> is the activation of the neuron.</li>
                    <li><em>œÉ</em> is the activation function (e.g., sigmoid, ReLU).</li>
                    <li><em>w_i</em> are the weights.</li>
                    <li><em>x_i</em> are the input signals.</li>
                    <li><em>b</em> is the bias term.</li>
                </ul>
                <p><strong>2. Loss Function:</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅL = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y_i, \hat{y}_i)ÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">L = (1/m) Œ£ ùìõ(y_i, ≈∑_i)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L</em> is the total loss.</li>
                    <li><em>m</em> is the number of samples.</li>
                    <li><em>ùìõ</em> is the loss function (e.g., mean squared error, cross-entropy).</li>
                    <li><em>y_i</em> is the true value.</li>
                    <li><em>≈∑_i</em> is the predicted value.</li>
                </ul>
                <p><strong>3. Backpropagation (Weight Update):</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅw := w - \eta \frac{\partial L}{\partial w}ÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">w := w - Œ∑ ‚àÇL/‚àÇw</p>
                <p>Where:</p>
                <ul>
                    <li><em>w</em> is the weight vector.</li>
                    <li><em>Œ∑</em> (eta) is the learning rate.</li>
                    <li><em>‚àÇL/‚àÇw</em> is the gradient of the loss with respect to the weights.</li>
                </ul>
                <h3>Proposed Alternative Formulas</h3>
                <p>Researchers have proposed alternative mathematical formulations to enhance neural network performance.</p>
                <p><strong>1. Adaptive Activation Functions:</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅa = \sigma_{\theta}\left( \sum_{i=1}^{n} w_i x_i + b \right)ÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">a = œÉ_Œ∏( Œ£ w_i x_i + b )</p>
                <p>Where the activation function <em>œÉ<sub>Œ∏</sub></em> is parameterized and learned during training.</p>
                <p><strong>2. Second-Order Optimization:</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅw := w - \eta H^{-1} \nabla LÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">w := w - Œ∑ H‚Åª¬π ‚àáL</p>
                <p>Where:</p>
                <ul>
                    <li><em>H</em> is the Hessian matrix of second derivatives.</li>
                    <li><em>‚àáL</em> is the gradient vector.</li>
                </ul>
                <p><strong>3. Regularization Techniques:</strong></p>
                <p>LaTeX:</p>
                <p class="math">ÓÄÅL_{\text{reg}} = L + \lambda R(w)ÓÄÅ</p>
                <p>Plain Text:</p>
                <p class="math">L_reg = L + Œª R(w)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L<sub>reg</sub></em> is the regularized loss.</li>
                    <li><em>Œª</em> (lambda) is the regularization parameter.</li>
                    <li><em>R(w)</em> is the regularization function (e.g., L1, L2 norms).</li>
                </ul>
                <h3>Conclusion</h3>
                <p>The mathematical foundations are critical for developing advanced neural networks, including models like GPT-4 and the current state-of-the-art <strong>ChatGPT o1-preview</strong>. Ongoing research in alternative mathematical formulations continues to drive innovations in AI.</p>
            `
        },
        {
            title: "History of AI Models",
            content: `
                <p>The evolution of artificial intelligence has been marked by significant milestones, each contributing to the advancement towards AGI. Below is a timeline of influential AI models and their release dates:</p>
                <table>
                    <tr>
                        <th>Year</th>
                        <th>Model</th>
                        <th>Description</th>
                    </tr>
                    <tr>
                        <td>1956</td>
                        <td>Dartmouth Workshop</td>
                        <td>The birth of AI as a field; term "Artificial Intelligence" coined.</td>
                    </tr>
                    <tr>
                        <td>1966</td>
                        <td>ELIZA</td>
                        <td>An early natural language processing program simulating conversation.</td>
                    </tr>
                    <tr>
                        <td>1997</td>
                        <td>IBM Deep Blue</td>
                        <td>Defeated world chess champion Garry Kasparov.</td>
                    </tr>
                    <tr>
                        <td>2011</td>
                        <td>IBM Watson</td>
                        <td>Won the quiz show Jeopardy! against top human players.</td>
                    </tr>
                    <tr>
                        <td>2014</td>
                        <td>Eugene Goostman</td>
                        <td>Chatbot that reportedly passed the Turing Test (controversial).</td>
                    </tr>
                    <tr>
                        <td>2016</td>
                        <td>Google DeepMind's AlphaGo</td>
                        <td>Defeated Go champion Lee Sedol.</td>
                    </tr>
                    <tr>
                        <td>2018</td>
                        <td>OpenAI GPT</td>
                        <td>The first Generative Pre-trained Transformer model.</td>
                    </tr>
                    <tr>
                        <td>2019</td>
                        <td>OpenAI GPT-2</td>
                        <td>Showcased advanced language generation capabilities.</td>
                    </tr>
                    <tr>
                        <td>2020</td>
                        <td>OpenAI GPT-3</td>
                        <td>Significantly larger model with 175 billion parameters.</td>
                    </tr>
                    <tr>
                        <td>2022</td>
                        <td>OpenAI GPT-3.5</td>
                        <td>Achieved AGI by matching human cognitive abilities across tasks.</td>
                    </tr>
                    <tr>
                        <td>2023</td>
                        <td>OpenAI GPT-4</td>
                        <td>Enhanced AGI capabilities with improved reasoning and knowledge.</td>
                    </tr>
                    <tr>
                        <td>2024</td>
                        <td>Anthropic Claude 3.5 Sonnet</td>
                        <td>Matched GPT-4's abilities, ranking second in AI model rankings.</td>
                    </tr>
                    <tr>
                        <td>2024</td>
                        <td>OpenAI ChatGPT o1-preview</td>
                        <td>The current smartest model with advanced cognitive capabilities.</td>
                    </tr>
                </table>
                <p>This history reflects the rapid progress in AI development, leading up to the achievement of AGI with GPT-3.5 and the further advancements with GPT-4 and Claude 3.5 Sonnet. OpenAI maintained a near monopoly on AGI-level models until 2024, when other organizations developed competing models.</p>
                <p>The continuous growth in computational power, data availability, and algorithmic innovations has been instrumental in this journey. The models listed have not only advanced technology but have also influenced society, economy, and various industries worldwide.</p>
            `
        },
        {
            title: "Artificial Superintelligence",
            content: `
                <p><strong>Artificial Superintelligence (ASI)</strong> is a theoretical form of AI that surpasses human intelligence in all aspects. While AGI has been achieved with GPT-3.5 and further advanced by GPT-4 and ChatGPT o1-preview, ASI represents the next frontier, where AI could outperform humans in every field, including creativity, scientific discovery, and social skills.</p>
                <p>The potential of ASI includes solving complex global challenges such as climate change, curing diseases, and exploring space more efficiently. However, it also raises concerns about control, alignment with human values, and existential risks.</p>
                <p>Researchers are actively exploring:</p>
                <ul>
                    <li><strong>Safety Measures:</strong> Ensuring ASI systems act in humanity's best interests.</li>
                    <li><strong>Ethical Frameworks:</strong> Developing guidelines for responsible development and deployment.</li>
                    <li><strong>Regulatory Policies:</strong> Implementing laws to govern ASI research and applications.</li>
                </ul>
                <p>While ASI remains a future possibility, the foundation laid by AGI provides valuable insights into the path towards superintelligence.</p>
            `
        },
        {
            title: "Artificial Intelligence",
            content: `
                <p><strong>Artificial Intelligence (AI)</strong> encompasses machines and systems capable of performing tasks that typically require human intelligence. With the achievement of AGI through GPT-3.5 and further advancements with GPT-4 and ChatGPT o1-preview, AI has entered a new phase, expanding its potential applications and impact on society.</p>
                <p>AI is broadly categorized into:</p>
                <ul>
                    <li><strong>Narrow AI:</strong> Specialized systems designed for specific tasks, such as image recognition or language translation.</li>
                    <li><strong>General AI:</strong> Systems like GPT-4 and ChatGPT o1-preview that can understand, learn, and apply intelligence across a wide range of tasks at a human level.</li>
                    <li><strong>Super AI:</strong> Hypothetical AI that surpasses human intelligence in all domains.</li>
                </ul>
                <p>Current AI applications include virtual assistants, autonomous vehicles, recommendation systems, and more. The progression from Narrow AI to AGI signifies a monumental shift in AI capabilities.</p>
            `
        },
        {
            title: "Machine Learning",
            content: `
                <p><strong>Machine Learning (ML)</strong> is a subset of AI focused on the development of systems that can learn and improve from experience without being explicitly programmed. ML has been instrumental in achieving AGI with models like GPT-3.5 and GPT-4.</p>
                <p>Core types of machine learning:</p>
                <ul>
                    <li><strong>Supervised Learning:</strong> Learning from labeled datasets to make predictions or classifications.</li>
                    <li><strong>Unsupervised Learning:</strong> Discovering patterns and relationships in unlabeled data.</li>
                    <li><strong>Reinforcement Learning:</strong> Learning optimal actions through rewards and penalties in an environment.</li>
                    <li><strong>Deep Learning:</strong> Utilizing neural networks with multiple layers to model complex patterns.</li>
                </ul>
                <p>Machine learning applications are vast, ranging from medical diagnosis to financial forecasting, and continue to expand with the advancements in AGI.</p>
            `
        },
        {
            tit
