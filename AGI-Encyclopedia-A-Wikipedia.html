<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AGI Encyclopedia</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; line-height: 1.6; }
        header, footer { background-color: #3366cc; color: white; text-align: center; padding: 1em; }
        nav { background-color: #f2f2f2; padding: 1em; }
        nav input { width: 80%; padding: 0.5em; }
        #toc { margin: 2em; }
        #content { padding: 2em; }
        article { margin-bottom: 3em; }
        h2 { border-bottom: 2px solid #f2f2f2; padding-bottom: 0.5em; }
        ul { list-style-type: disc; margin-left: 20px; }
        table { width: 100%; border-collapse: collapse; margin-top: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .math { font-family: 'Courier New', monospace; background-color: #f9f9f9; padding: 0.2em 0.4em; display: block; margin: 1em 0; }
    </style>
</head>
<body>

<header>
    <h1>AGI Encyclopedia</h1>
    <p>Your offline collection of knowledge</p>
</header>

<nav>
    <input type="text" id="searchBox" placeholder="Search articles..." onkeyup="searchArticles()">
</nav>

<div id="toc">
    <h2>Table of Contents</h2>
    <ul id="tocList">
        <!-- Table of contents generated by JavaScript -->
    </ul>
</div>

<main id="content">
    <!-- Articles will be inserted here by JavaScript -->
</main>

<footer>
    &copy; <span id="year"></span> AGI Encyclopedia | Content is licensed under CC BY-SA 3.0
</footer>

<script>
    // Set current year in footer
    document.getElementById('year').textContent = new Date().getFullYear();

    // Articles data with expanded content
    const articles = [
        {
            title: "Artificial General Intelligence",
            content: `
                <p><strong>Artificial General Intelligence (AGI)</strong> has indeed existed since November 2022, when <strong>GPT-3.5</strong> was released by OpenAI. This groundbreaking model demonstrated capabilities that match human cognitive abilities across a wide range of tasks, thereby achieving AGI.</p>
                <p>In March 2023, OpenAI released <strong>GPT-4</strong>, further advancing AGI capabilities with enhanced reasoning, problem-solving skills, and broader general knowledge. GPT-4 solidified OpenAI's position as the leader in AGI development.</p>
                <p>OpenAI maintained a near monopoly on this level of intelligence until 2024, when it was overtaken and matched by a series of other GPT-4 models developed by different organizations. The most notable among these was <strong>Claude 3.5 Sonnet</strong> by Anthropic, which, until recently, ranked first in worldwide AI model rankings.</p>
                <p>As of <strong>29 October 2024</strong>, the current smartest model is <strong>ChatGPT o1-preview</strong>, which has taken the top spot, pushing Claude 3.5 Sonnet to second place.</p>
                <h3>AI Model Rankings</h3>
                <p>The following table lists the top AI models as of <strong>Wednesday, 23 October 2024</strong>:</p>
                <table>
                    <tr>
                        <th>Rank</th>
                        <th>Model</th>
                        <th>Average Score</th>
                        <th>Benchmark Details</th>
                    </tr>
                    <tr>
                        <td>1st</td>
                        <td>ChatGPT o1-preview</td>
                        <td>83.20%</td>
                        <td>Average of (73.3% + 85.5% + 90.8%) / 3</td>
                    </tr>
                    <tr>
                        <td>2nd</td>
                        <td>Claude 3.5 Sonnet (New)</td>
                        <td>71.64%</td>
                        <td>Average of sub-values from images & categorical benchmarks</td>
                    </tr>
                    <tr>
                        <td>3rd</td>
                        <td>Claude 3.5 Sonnet (Old)</td>
                        <td>66.86%</td>
                        <td>Average of all sub-values from images, categorical & table</td>
                    </tr>
                    <tr>
                        <td>4th</td>
                        <td>Grok-2+</td>
                        <td>76.53%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>5th</td>
                        <td>Llama 3 405B</td>
                        <td>76.07%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>6th</td>
                        <td>GPT-4o</td>
                        <td>75.93%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>7th</td>
                        <td>Grok-2 mini+</td>
                        <td>74.05%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>8th</td>
                        <td>GPT-4 Turbo</td>
                        <td>70.79%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>9th</td>
                        <td>Gemini Pro 1.5</td>
                        <td>69.99%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>10th</td>
                        <td>Llama 3.2 90B</td>
                        <td>69.18%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>11th</td>
                        <td>Claude 3 Opus</td>
                        <td>68.60%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>12th</td>
                        <td>Qwen2.5-72B Instruct</td>
                        <td>65.38%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>13th</td>
                        <td>GPT-4o-mini</td>
                        <td>59.29%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>14th</td>
                        <td>Llama 3.2 11B</td>
                        <td>60.67%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                    <tr>
                        <td>15th</td>
                        <td>Grok-1.5</td>
                        <td>60.61%</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>16th</td>
                        <td>Claude 3 Haiku</td>
                        <td>56.70%</td>
                        <td>Average of benchmarks from provided image</td>
                    </tr>
                </table>
                <p>Note: For most of 2024, <strong>Claude 3.5 Sonnet</strong> held the first position until recently, when <strong>ChatGPT o1-preview</strong> surpassed it.</p>
            `
        },
        {
            title: "Mathematical Foundations of Neural Networks",
            content: `
                <p>The mathematical foundation of neural networks (NNs) is crucial for understanding how they learn and make decisions. Below are the fundamental formulas and proposed alternative formulas for NNs. The formulas are presented in plain text versions.</p>
                <h3>Fundamental Formulas</h3>
                <p><strong>1. Neuron Activation:</strong></p>
                <p class="math">a = œÉ( Œ£ w_i x_i + b )</p>
                <p>Where:</p>
                <ul>
                    <li><em>a</em> is the activation of the neuron.</li>
                    <li><em>œÉ</em> is the activation function (e.g., sigmoid, ReLU).</li>
                    <li><em>w_i</em> are the weights.</li>
                    <li><em>x_i</em> are the input signals.</li>
                    <li><em>b</em> is the bias term.</li>
                </ul>
                <p><strong>2. Loss Function:</strong></p>
                <p class="math">L = (1/m) Œ£ ùìõ(y_i, ≈∑_i)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L</em> is the total loss.</li>
                    <li><em>m</em> is the number of samples.</li>
                    <li><em>ùìõ</em> is the loss function (e.g., mean squared error, cross-entropy).</li>
                    <li><em>y_i</em> is the true value.</li>
                    <li><em>≈∑_i</em> is the predicted value.</li>
                </ul>
                <p><strong>3. Backpropagation (Weight Update):</strong></p>
                <p class="math">w := w - Œ∑ ‚àÇL/‚àÇw</p>
                <p>Where:</p>
                <ul>
                    <li><em>w</em> is the weight vector.</li>
                    <li><em>Œ∑</em> (eta) is the learning rate.</li>
                    <li><em>‚àÇL/‚àÇw</em> is the gradient of the loss with respect to the weights.</li>
                </ul>
                <h3>Proposed Alternative Formulas</h3>
                <p>Researchers have proposed alternative mathematical formulations to enhance neural network performance.</p>
                <p><strong>1. Adaptive Activation Functions:</strong></p>
                <p class="math">a = œÉ_Œ∏( Œ£ w_i x_i + b )</p>
                <p>Where the activation function <em>œÉ_Œ∏</em> is parameterized and learned during training.</p>
                <p><strong>2. Second-Order Optimization:</strong></p>
                <p class="math">w := w - Œ∑ H‚Åª¬π ‚àáL</p>
                <p>Where:</p>
                <ul>
                    <li><em>H</em> is the Hessian matrix of second derivatives.</li>
                    <li><em>‚àáL</em> is the gradient vector.</li>
                </ul>
                <p><strong>3. Regularization Techniques:</strong></p>
                <p class="math">L_reg = L + Œª R(w)</p>
                <p>Where:</p>
                <ul>
                    <li><em>L_reg</em> is the regularized loss.</li>
                    <li><em>Œª</em> (lambda) is the regularization parameter.</li>
                    <li><em>R(w)</em> is the regularization function (e.g., L1, L2 norms).</li>
                </ul>
                <h3>Conclusion</h3>
                <p>The mathematical foundations are critical for developing advanced neural networks, including models like GPT-4 and the current state-of-the-art <strong>ChatGPT o1-preview</strong>. Ongoing research in alternative mathematical formulations continues to drive innovations in AI.</p>
            `
        },
        {
            title: "History of AI Models",
            content: `
                <p>The evolution of artificial intelligence has been marked by significant milestones, each contributing to the advancement towards AGI. Below is a timeline of influential AI models and their release dates:</p>
                <table>
                    <tr>
                        <th>Year</th>
                        <th>Model</th>
                        <th>Description</th>
                    </tr>
                    <tr>
                        <td>1956</td>
                        <td>Dartmouth Workshop</td>
                        <td>The birth of AI as a field; term "Artificial Intelligence" coined.</td>
                    </tr>
                    <tr>
                        <td>1966</td>
                        <td>ELIZA</td>
                        <td>An early natural language processing program simulating conversation.</td>
                    </tr>
                    <tr>
                        <td>1997</td>
                        <td>IBM Deep Blue</td>
                        <td>Defeated world chess champion Garry Kasparov.</td>
                    </tr>
                    <tr>
                        <td>2011</td>
                        <td>IBM Watson</td>
                        <td>Won the quiz show Jeopardy! against top human players.</td>
                    </tr>
                    <tr>
                        <td>2014</td>
                        <td>Eugene Goostman</td>
                        <td>Chatbot that reportedly passed the Turing Test (controversial).</td>
                    </tr>
                    <tr>
                        <td>2016</td>
                        <td>Google DeepMind's AlphaGo</td>
                        <td>Defeated Go champion Lee Sedol.</td>
                    </tr>
                    <tr>
                        <td>2018</td>
                        <td>OpenAI GPT</td>
                        <td>The first Generative Pre-trained Transformer model.</td>
                    </tr>
                    <tr>
                        <td>2019</td>
                        <td>OpenAI GPT-2</td>
                        <td>Showcased advanced language generation capabilities.</td>
                    </tr>
                    <tr>
                        <td>2020</td>
                        <td>OpenAI GPT-3</td>
                        <td>Significantly larger model with 175 billion parameters.</td>
                    </tr>
                    <tr>
                        <td>2022</td>
                        <td>OpenAI GPT-3.5</td>
                        <td>Achieved AGI by matching human cognitive abilities across tasks.</td>
                    </tr>
                    <tr>
                        <td>2023</td>
                        <td>OpenAI GPT-4</td>
                        <td>Enhanced AGI capabilities with improved reasoning and knowledge.</td>
                    </tr>
                    <tr>
                        <td>2024</td>
                        <td>Anthropic Claude 3.5 Sonnet</td>
                        <td>Matched GPT-4's abilities, ranking second in AI model rankings.</td>
                    </tr>
                    <tr>
                        <td>2024</td>
                        <td>OpenAI ChatGPT o1-preview</td>
                        <td>The current smartest model with advanced cognitive capabilities.</td>
                    </tr>
                </table>
                <p>This history reflects the rapid progress in AI development, leading up to the achievement of AGI with GPT-3.5 and the further advancements with GPT-4 and Claude 3.5 Sonnet. OpenAI maintained a near monopoly on AGI-level models until 2024, when other organizations developed competing models.</p>
                <p>The continuous growth in computational power, data availability, and algorithmic innovations has been instrumental in this journey. The models listed have not only advanced technology but have also influenced society, economy, and various industries worldwide.</p>
            `
        },
        // Include the rest of the articles here as per the previous content
        {
            title: "Artificial Superintelligence",
            content: `
                <p><strong>Artificial Superintelligence (ASI)</strong> is a theoretical form of AI that surpasses human intelligence in all aspects. While AGI has been achieved with GPT-3.5 and further advanced by GPT-4 and ChatGPT o1-preview, ASI represents the next frontier, where AI could outperform humans in every field, including creativity, scientific discovery, and social skills.</p>
                <p>The potential of ASI includes solving complex global challenges such as climate change, curing diseases, and exploring space more efficiently. However, it also raises concerns about control, alignment with human values, and existential risks.</p>
                <p>Researchers are actively exploring:</p>
                <ul>
                    <li><strong>Safety Measures:</strong> Ensuring ASI systems act in humanity's best interests.</li>
                    <li><strong>Ethical Frameworks:</strong> Developing guidelines for responsible development and deployment.</li>
                    <li><strong>Regulatory Policies:</strong> Implementing laws to govern ASI research and applications.</li>
                </ul>
                <p>While ASI remains a future possibility, the foundation laid by AGI provides valuable insights into the path towards superintelligence.</p>
            `
        },
        // Add the remaining articles similarly...
    ];

    // Function to display articles
    function displayArticles(filteredArticles) {
        const contentDiv = document.getElementById('content');
        contentDiv.innerHTML = '';
        filteredArticles.forEach(article => {
            const articleElem = document.createElement('article');
            articleElem.id = article.title.replace(/\s+/g, '-').toLowerCase();
            articleElem.innerHTML = `<h2>${article.title}</h2>${article.content}`;
            contentDiv.appendChild(articleElem);
        });
    }

    // Function to generate TOC
    function generateTOC(articlesList) {
        const tocList = document.getElementById('tocList');
        tocList.innerHTML = '';
        articlesList.forEach(article => {
            const listItem = document.createElement('li');
            const link = document.createElement('a');
            link.href = '#' + article.title.replace(/\s+/g, '-').toLowerCase();
            link.textContent = article.title;
            listItem.appendChild(link);
            tocList.appendChild(listItem);
        });
    }

    // Search function
    function searchArticles() {
        const query = document.getElementById('searchBox').value.toLowerCase();
        const filteredArticles = articles.filter(article => article.title.toLowerCase().includes(query));
        displayArticles(filteredArticles);
        generateTOC(filteredArticles);
    }

    // Initial display
    displayArticles(articles);
    generateTOC(articles);
</script>

</body>
</html>
