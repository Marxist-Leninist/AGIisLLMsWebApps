<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Massive GPT-Type Transformer Equation (Low-Rank Factorized Attention)</title>
<style>
body {
  font-family: sans-serif;
  margin: 20px;
}
code {
  white-space: pre;
  font-family: monospace;
}
</style>
</head>
<body>

<h1>GPT-Type Transformer with Low-Rank Factorized Attention - Massive Equation</h1>

<p>This document represents the entire forward pass equation of a GPT-type Transformer with enforced low-rank attention factorization. We have described all the steps and parameters above. Here we show a final integrated formula for the probability of the next token, along with an image representation.</p>

<h2>Final Integration</h2>

<p>After computing <code>Z^(N)(W)</code> through all N layers as previously defined, we project to the vocabulary space and apply softmax:</p>

<p>Then :)</p>
<p><img src="https://quicklatex.com/cache3/0c/ql_de2c69b69498dab2a58a3336d0fc520c_l3.png" alt="P(w_{t+1}=v|W) = \frac{\exp\left(\sum_{f=1}^{d_{\text{model}}}[Z^{(N)}(W)]_{t,f}(W_{\text{vocab}})_{f,v} + b_{\text{vocab},v}\right)}{\sum_{v'=1}^{|V|}\exp\left(\sum_{f=1}^{d_{\text{model}}}[Z^{(N)}(W)]_{t,f}(W_{\text{vocab}})_{f,v'} + b_{\text{vocab},v'}\right)}"/> </p>

<p>This matches the final formula for the probability distribution over the next token <code>w_{t+1}</code>, computed after applying all layers and transformations described in the large expanded equation.</p>

</body>
</html>
